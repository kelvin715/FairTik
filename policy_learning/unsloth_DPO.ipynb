{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One must patch the DPO Trainer first!\n",
    "from unsloth import PatchDPOTrainer\n",
    "\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88807a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/gz-fs/models/Qwen3-8B\", # Choose ANY! eg mistralai/Mistral-7B-Instruct-v0.2\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc26d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split dataset \n",
    "\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load original data\n",
    "dpo_dataset_path = \"/root/tiktok_techjam/policy_learning/dataset/dpo_dataset_hf.json\"\n",
    "with open(dpo_dataset_path, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Process data format\n",
    "processed_data = []\n",
    "for item in raw_data:\n",
    "    processed_data.append({\n",
    "        \"prompt\": item[\"prompt\"],\n",
    "        \"chosen\": item[\"chosen\"],\n",
    "        \"rejected\": item[\"rejected\"]\n",
    "    })\n",
    "\n",
    "print(f\"Total dataset size: {len(processed_data)}\")\n",
    "\n",
    "# Dataset split (80% tra    in, 20% eval)\n",
    "train_data, eval_data = train_test_split(\n",
    "    processed_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_data)}\")\n",
    "print(f\"Eval dataset size: {len(eval_data)}\")\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(eval_data)\n",
    "\n",
    "# Create raw_datasets dictionary\n",
    "raw_datasets = {\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": eval_dataset\n",
    "}\n",
    "\n",
    "print(\"Dataset split and loading completed!\")\n",
    "print(f\"Train dataset columns: {train_dataset.column_names}\")\n",
    "print(f\"Train dataset sample:\")\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c63b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset status\n",
    "print(\"=== Dataset Information ===\")\n",
    "print(f\"Training dataset: {train_dataset}\")\n",
    "print(f\"Evaluation dataset: {eval_dataset}\")\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "print(\"Training sample:\")\n",
    "for key, value in train_dataset[0].items():\n",
    "    print(f\"{key}: {value[:100]}...\" if len(str(value)) > 100 else f\"{key}: {value}\")\n",
    "    \n",
    "print(f\"\\nDataset ready for DPO training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f86fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Currently only supports dropout = 0\n",
    "    bias = \"none\",    # Currently only supports bias = \"none\"\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "# 初始化DPO训练器\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None,\n",
    "    args = DPOConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 10,\n",
    "        learning_rate = 5e-6,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.0,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    "    beta = 0.1,\n",
    "    train_dataset = raw_datasets[\"train\"],\n",
    "    eval_dataset = raw_datasets[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 1024,\n",
    "    max_prompt_length = 512,\n",
    ")\n",
    "\n",
    "print(\"DPO Trainer initialized successfully!\")\n",
    "print(f\"Training samples: {len(raw_datasets['train'])}\")\n",
    "print(f\"Evaluation samples: {len(raw_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DPO training\n",
    "print(\"Starting DPO training...\")\n",
    "print(f\"Total training steps: {len(train_dataset) // (2 * 4) * 3}\")  # batch_size * grad_accum * epochs\n",
    "\n",
    "# Train model\n",
    "trainer_output = dpo_trainer.train()\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final training loss: {trainer_output.training_loss}\")\n",
    "\n",
    "# Save final model\n",
    "print(\"Saving final model...\")\n",
    "dpo_trainer.save_model(\"./final_dpo_model\")\n",
    "tokenizer.save_pretrained(\"./final_dpo_model\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a49935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.10: Fast Qwen3 patching. Transformers: 4.55.4.\n",
      "   \\\\   /|    NVIDIA A100-PCIE-40GB. Num GPUs = 1. Max memory: 39.495 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbbb69a903941ac82223270a7600877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the train model\n",
    "\n",
    "model_name = \"/root/tiktok_techjam/policy_learning/final_dpo_model\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=4096,\n",
    "    dtype=\"bfloat16\",\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cb008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作为TikTok收益分配专家，请为以下创作者推荐一个公平的收益分成比例。\n",
      "\n",
      "创作者档案：\n",
      "- 创作经验：1个月\n",
      "- 粉丝数量：50000\n",
      "- 历史表现：0.57\n",
      "- 稳定性评分：0.37\n",
      "\n",
      "内容质量评估：\n",
      "- 内容价值：0.83\n",
      "- 视觉质量：0.59\n",
      "- 主播表现：0.66\n",
      "- 整体评分：0.69\n",
      "\n",
      "互动表现：\n",
      "- 观看人数：14574\n",
      "- 参与率：0.12\n",
      "- 留存率：0.58\n",
      "- 聊天活跃度：0.44\n",
      "\n",
      "基础数据：\n",
      "- 直播时长：120分钟\n",
      "- 峰值观众：14574\n",
      "- 平均观众：12515\n",
      "- 评论数：23523\n",
      "- 点赞数：29275\n",
      "\n",
      "提供一个公平的创作者收益分成比例(0-100%)和简短的理由，以及建议的改进"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方向。\n",
      "作为TikTok收益分配专家，请为以下创作者推荐一个公平的收益分成比例。\n",
      "\n",
      "创作者档案：\n",
      "- 创作经验：1个月\n",
      "- 絲粉數量：50000\n",
      "- 歷史表現：0.57\n",
      "- 穩定性評分：0.37\n",
      "\n",
      "內容質量評估：\n",
      "- 內容價值：0.83\n",
      "- 美術質量：0.59\n",
      "- 主播表現：0.66\n",
      "- 整體評分：0.69\n",
      "\n",
      "互動表現：\n",
      "- 觀看人數：14574\n",
      "- 參與率：0.12\n",
      "- 留存率：0.58\n",
      "- 聊天活躍度：0.44\n",
      "\n",
      "基礎數據：\n",
      "- 直播時長：120分鐘\n",
      "- 峰值觀眾：14574\n",
      "- 平均觀眾：12515\n",
      "- 評論數：23523\n",
      "- 点赞数：29275\n",
      "\n",
      "提供一个公平的创作者收益分成比例(0-100%)和简短的理由，以及建议的改进方向。\n",
      "\n",
      "好的，我现在需要帮用户解决一个关于TikTok创作者收益分成比例的问题。首先，我得仔细看看用户提供的所有数据，然后根据这些数据来推荐一个公平的比例，同时还要给出理由和改进建议。\n",
      "\n",
      "首先，用户提供的数据包括创作者的经验、粉丝数量、历史表现、稳定性评分，还有内容质量评估、互动表现和基础数据。我需要把这些数据综合起来分析。\n",
      "\n",
      "创作者有1个月的经验，粉丝数5万，这说明他可能是一个新晋的创作者，虽然粉丝不算太少，但经验不足。历史表现是0.57，稳定性评分0.37，这两个分数可能指的是某些评分系统中的数值，比如TikTok的内部评分，但具体标准我不太清楚，不过通常0.57可能属于中等水平，而稳定性评分0.37可能较低，说明直播的稳定性不够好，可能有中断或者不规律的情况。\n",
      "\n",
      "内容质量方面，内容价值0.83，这还不错，说明内容有较高的价值；美术质量0.59，可能在视觉效果上还有提升空间；主播表现0.66，整体评分0\n"
     ]
    }
   ],
   "source": [
    "# inference on the test dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\"作为TikTok收益分配专家，请为以下创作者推荐一个公平的收益分成比例。\\n\\n创作者档案：\\n- 创作经验：1个月\\n- 粉丝数量：50000\\n- 历史表现：0.57\\n- 稳定性评分：0.37\\n\\n内容质量评估：\\n- 内容价值：0.83\\n- 视觉质量：0.59\\n- 主播表现：0.66\\n- 整体评分：0.69\\n\\n互动表现：\\n- 观看人数：14574\\n- 参与率：0.12\\n- 留存率：0.58\\n- 聊天活跃度：0.44\\n\\n基础数据：\\n- 直播时长：120分钟\\n- 峰值观众：14574\\n- 平均观众：12515\\n- 评论数：23523\\n- 点赞数：29275\\n\\n提供一个公平的创作者收益分成比例(0-100%)和简短的理由\"],\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "FastLanguageModel.for_inference(model) \n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e07dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
